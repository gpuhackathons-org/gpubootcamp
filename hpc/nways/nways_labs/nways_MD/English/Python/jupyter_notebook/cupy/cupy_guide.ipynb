{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# \n",
    "\n",
    "#  CuPy Lab 1\n",
    "---\n",
    "Before we begin, let's execute the cell below to display information about the CUDA driver and GPUs running on the server by running the `nvidia-smi` command. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above. If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Learning Objectives\n",
    "- **The goal of this lab is to:**\n",
    "     - quickly get you started with CuPy from beginner to intermediate level\n",
    "     - teach you application of GPU programming concept to HPC field(s)\n",
    "     - show you how to maximize the throughput of your HPC implementation through computational speedup on the GPU.      \n",
    "\n",
    "\n",
    "##  What is CuPy?\n",
    "- CuPy is an implementation of NumPy-compatible multi-dimensional array on CUDA\n",
    "- CuPy consists of :\n",
    "    - cupy.ndarray \n",
    "    - the core multi-dimensional array class \n",
    "    - many functions \n",
    "- It supports a subset of numpy.ndarray interface which include:\n",
    "    - Basic and advance indexing \n",
    "    - Data types (int32, float32, uint64, complex64,... )\n",
    "    - Array manipulation routine (reshape)\n",
    "    - Linear Algebra functions (dot, matmul, etc)\n",
    "    - Reduction along axis (max, sum, argmax, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "#Basic indexing and slicing\n",
    "print(X[5:])\n",
    "\n",
    "#output: [5 6 7 8 9]\n",
    "\n",
    "print(X[1:7:2])\n",
    "\n",
    "#output: [1 3 5]\n",
    "\n",
    "#reduction and Linear Algebra function\n",
    "max(X)\n",
    "#output:  9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Advance indexing\n",
    "X = np.array([[1, 2],[3, 4],[5, 6]])\n",
    "print(X[[0, 1, 2], [0, 1, 0]])\n",
    "\n",
    "#output: [1 4 5]\n",
    "\n",
    "B = np.array([1,2,3,4], dtype=np.float32)\n",
    "C = np.array([5,6,7,8], dtype=np.float32)\n",
    "print(\"matmul:\",np.matmul(B, C))\n",
    "\n",
    "#output: matmul: 70.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#data type and array manipulation routine \n",
    "A =1j*np.arange(9, dtype=np.complex64).reshape(3,3)\n",
    "print(A)\n",
    "\n",
    "\n",
    "#output:\n",
    "#[[0.+0.j 0.+1.j 0.+2.j]\n",
    "# [0.+3.j 0.+4.j 0.+5.j]\n",
    "# [0.+6.j 0.+7.j 0.+8.j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Features of CuPy\n",
    "\n",
    "- **Features of CuPy includes:**\n",
    "    - User-define elementwise CUDA kernels\n",
    "    - User-define reduction CUDA kernels\n",
    "    - Fusing CUDA kernels to optimize user-define calculation\n",
    "    - Customizable memory allocator and memory pool\n",
    "    - cuDNN utilities\n",
    "- These features  are developed to support performance\n",
    "- CuPy uses on-the-fly kernel synthesis: when a kernel call is required, it compiles a kernel code optimized for the shapes and dtypes of given arguments, sends it to the GPU device, and executes the kernel.\n",
    "- CuPy also caches the kernel code sent to GPU device within the process, which reduces the kernel transfer time on further calls. \n",
    "\n",
    "\n",
    "###  CuPy Architecture\n",
    "\n",
    "The CuPy Architecture expose functionalities within CuPy API that allows developers (or users) to create a User-defined CuDA Kernel, make use of DNN utility through the cuDNN functionality, solve Linear algebra through cuTENSOR, cuBLAS, cuSOLVER, and cuSPARSE api functions. Also, Random numbers can be generated using cuRAND. Sort, Scan and Reduction operations are conveniently executed through the use of CUB and Thrust, and lastly, Multi-GPU data transfer task are initiated through the use of NCCL functionality. It important to know that all the aformentioned api functionalities rely on CUDA, while the CUDA itself runs \n",
    "on the NVIDIA GPU as shown in the diagram below. \n",
    "\n",
    "<img src=\"../images/cupy_arch.png\">\n",
    "\n",
    "\n",
    "## CuPy Fundamentals\n",
    "- **Cupy.ndarray**: CuPy is a GPU array backend that implements a subset of NumPy interface.\n",
    "```python\n",
    "#CuPy version\n",
    "import cupy as cp\n",
    "X_gpu = cp.array([1, 2, 3, 4, 5])\n",
    "#NumPy version\n",
    "import numpy as np\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "```\n",
    "\n",
    "- CuPy considers the current device as the default device with device ID 0. It also allows temporary switch between GPU devices.\n",
    "\n",
    "```python\n",
    "import cupy as cp\n",
    "####### Current device (GPU ID: 0)##############\n",
    "gpu_0 = cp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Switch device\n",
    "cp.cuda.Device(1).use()\n",
    "gpu_1 = cp.array([1, 2, 3, 4])\n",
    "\n",
    "###### Switch GPU temporarily################\n",
    "import numpy as np\n",
    "with cp.cuda.Device(1):\n",
    "      gpu_1 = cp.array([1, 2, 3, 4])\n",
    "# back to device id 0\n",
    "gpu0 = cp.array([1, 2, 3, 4, 5]) \n",
    "```\n",
    "\n",
    "### Data transfer\n",
    "- Arrays can be moved from Host to Device (CPU -> GPU) using **cupy.asarray**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "x_gpu = cp.asarray(x)\n",
    "print(x_gpu)\n",
    "\n",
    "#output: [1 2 3 4 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Device array can be move to Host(GPU -> CPU) using: **cupy.asnumpy or cupy.ndarray.get()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "x_gpu = cp.array([1, 2, 3, 4, 5])\n",
    "#copy to Host\n",
    "x_cpu = cp.asnumpy(x_gpu)\n",
    "print(\"x_cpu: \",x_cpu)\n",
    "\n",
    "#alternative option\n",
    "x_cpu_alt = x_gpu.get()\n",
    "print(\"x_cpu_alt: \",x_cpu_alt) \n",
    "\n",
    "#output: \n",
    "#x_cpu:  [1 2 3 4 5]\n",
    "#x_cpu_alt:  [1 2 3 4 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to transfer an array between devices(GPU to GPU), **cupy.ndarray** is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "with cp.cuda.Device(0):\n",
    "    x_gpu_0 = cp.ndarray([ 2, 3, 3]) \n",
    "print(\"x_gpu_0:\\n\", x_gpu_0)\n",
    "\n",
    "with cp.cuda.Device(0):\n",
    "      x_gpu_1 = cp.asarray(x_gpu_0)\n",
    "print(\"x_gpu_1:\\n\", x_gpu_1)\n",
    "\n",
    "#output\n",
    "#x_gpu_0:\n",
    "# [[[0. 0. 0.]\n",
    "#  [0. 0. 0.]\n",
    "#  [0. 0. 0.]]\n",
    "\n",
    "# [[0. 0. 0.]\n",
    "#  [0. 0. 0.]\n",
    "#  [0. 0. 0.]]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU  & CPU agnostic code\n",
    "- The compatibility of CuPy with NumPy enables the implementation of CPU/GPU generic code using **cupy.get_array_module()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "#example: log(1 + exp(x))\n",
    "x_cpu  = np.array([1, 2, 3, 4, 5])\n",
    "x_gpu  = cp.get_array_module(x_cpu)\n",
    "result = x_gpu.maximum(0, x_cpu) + x_gpu.log1p(x_gpu.exp(-abs(x_cpu)))\n",
    "print(result)\n",
    "\n",
    "#output: [1.31326169 2.12692801 3.04858735 4.01814993 5.00671535]\n",
    "\n",
    "#An explicit conversion to a host \n",
    "x_gpu  = cp.array([6, 7, 8, 9, 10])\n",
    "result = cp.asnumpy(x_gpu) + x_cpu\n",
    "print(result)\n",
    "\n",
    "#output: [ 7  9 11 13 15]\n",
    "\n",
    "#An explicit conversion to a device\n",
    "result = x_gpu + cp.asarray(x_cpu)\n",
    "print(result)\n",
    "\n",
    "#output: [ 7  9 11 13 15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CUDA Kernels\n",
    "\n",
    "- **CUDA Kernels can be define in Cupy as follows:**\n",
    "\n",
    "    - Elementwise Kernels\n",
    "    - Reduction Kernels\n",
    "    - Raw Kernels\n",
    "    - Kernel Fusion\n",
    "- These kernels are user-defined based.\n",
    "\n",
    "### Elementwise Kernels\n",
    "- The ElementwiseKernel class is used to define this type of kernel.\n",
    "- This kernel consists of four parts which includes:\n",
    "    1. a list of input argument \n",
    "    2. a list of output argument\n",
    "    3. a loop body code\n",
    "    4. kernel name\n",
    "- Variable name starting with underscore “_” , “n”, and “i” are regarded as reserved keywords.\n",
    "\n",
    "#### Example: z = x*w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "input_list = 'float32 x , float32 w, float32 b'\n",
    "output_list = 'float32 z'\n",
    "code_body  = 'z =  (x * w) + b'\n",
    "\n",
    "# elementwisekernel class defined\n",
    "dnnLayerNode = cp.ElementwiseKernel(input_list, output_list, code_body,'dnnLayerNode')\n",
    "\n",
    "# data\n",
    "x = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "w = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "b = cp.array([-0.5], dtype=cp.float32)\n",
    "z = cp.empty((3,3), dtype=cp.float32)\n",
    "\n",
    "# kernel call with argument passing\n",
    "dnnLayerNode(x,w,b,z)\n",
    "print(z)\n",
    "\n",
    "#output:\n",
    "#[[-0.5  0.5  3.5]\n",
    "# [ 8.5 15.5 24.5]\n",
    "# [35.5 48.5 63.5]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise Kernel: Generic-type kernels\n",
    "- It can be used to define a generic-type kernels. It treats a type specifier of one character as a type placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "input_list = 'T x , T w, T b'\n",
    "output_list = 'T z'\n",
    "code_body  = 'z =  (x * w) + b'\n",
    "\n",
    "# elementwisekernel class defined\n",
    "dnnLayerNode = cp.ElementwiseKernel(input_list, output_list, code_body,'dnnLayerNode')\n",
    "x = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "w = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "b = cp.array([-0.5], dtype=cp.float32)\n",
    "z = cp.empty((3,3), dtype=cp.float32)\n",
    "\n",
    "# kernel call with argument passing\n",
    "dnnLayerNode(x,w,b,z)\n",
    "print(z)\n",
    "\n",
    "#output:\n",
    "#[[-0.5  0.5  3.5]\n",
    "# [ 8.5 15.5 24.5]\n",
    "# [35.5 48.5 63.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Kernels\n",
    "\n",
    "- Reduction kernel is implemented through the ReductionKernel class. \n",
    "- In order to implement this kernel class, the following parts must be defined:\n",
    "    - **Identity value**: to initialize reduction value.\n",
    "    - **Mapping expression**: Used for the pre-processing of each element to be reduced.\n",
    "    - **Reduction expression**: It is an operator to reduce the multiple mapped values. The special variables **a** and **b** are used for its operands.\n",
    "    - **Post mapping expression**: It is used to transform the resulting reduced values. The special variable a is used as its input. Output should be written to the output parameter.\n",
    "\n",
    "\n",
    "**Example:  𝑧=∑_(𝑖=1)𝑥_𝑖 *𝑤_𝑖+𝑏**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "dnnLayer = cp.ReductionKernel(\n",
    "\t'T x, T w, T bias',              \n",
    "\t'T z',                         \n",
    "\t'x * w',                      \n",
    "\t'a + b', \n",
    "\t'z = a + bias',              \n",
    "\t'0',                            \n",
    "\t'dnnLayer'  )\n",
    "x = cp.arange(10, dtype=np.float32).reshape(2,5)\n",
    "w = cp.arange(10, dtype=np.float32).reshape(2,5)\n",
    "bias = -0.1\n",
    "z = dnnLayer(x,w,bias)\n",
    "print(z)\n",
    "\n",
    "#output: 284.9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Kernels\n",
    "\n",
    "- Raw kernels enables  the  direct use of kernels from CUDA source, and it is defined through the RawKernel class.\n",
    "- The RawKernel object allows you to call the kernel with CUDA’s cuLaunchKernel interface. In other words, you have control over:\n",
    "    - grid size\n",
    "    - block size\n",
    "    - shared memory size \n",
    "    - and stream. \n",
    "<img src=\"../images/cupy_kernel_memory.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "add_kernel = cp.RawKernel(r'''\n",
    "extern \"C\" __global__\n",
    "void add_func(const float* x1, const float* x2, float* y) {\n",
    "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "y[tid] = x1[tid] + x2[tid];\n",
    "}\n",
    "''', 'add_func')\n",
    "\n",
    "N = 100\n",
    "shape = (10, 10)\n",
    "\n",
    "x1 = cp.arange(N, dtype=cp.float32).reshape(shape)\n",
    "x2 = cp.arange(N, dtype=cp.float32).reshape(shape)\n",
    "y = cp.zeros((shape), dtype=cp.float32)\n",
    "\n",
    "add_kernel((10,), (10,), (x1, x2, y)) \n",
    "\n",
    "print(y)\n",
    "\n",
    "#output:\n",
    "#[[  0.   2.   4.   6.   8.  10.  12.  14.  16.  18.]\n",
    "#[ 20.  22.  24.  26.  28.  30.  32.  34.  36.  38.]\n",
    "#[ 40.  42.  44.  46.  48.  50.  52.  54.  56.  58.]\n",
    "#[ 60.  62.  64.  66.  68.  70.  72.  74.  76.  78.]\n",
    "#[ 80.  82.  84.  86.  88.  90.  92.  94.  96.  98.]\n",
    "#[100. 102. 104. 106. 108. 110. 112. 114. 116. 118.]\n",
    "#[120. 122. 124. 126. 128. 130. 132. 134. 136. 138.]\n",
    "#[140. 142. 144. 146. 148. 150. 152. 154. 156. 158.]\n",
    "#[160. 162. 164. 166. 168. 170. 172. 174. 176. 178.]\n",
    "#[180. 182. 184. 186. 188. 190. 192. 194. 196. 198.]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/raw_kernel.png\">\n",
    "\n",
    "#### Raw Kernels : Complex-value arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "complex_kernel = cp.RawKernel(r'''\n",
    "#include <cupy/complex.cuh>\n",
    "extern \"C\" __global__\n",
    "void my_func(const complex<float>* x1, const complex<float>* x2, complex<float>* y, float a){\n",
    "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    y[tid] = x1[tid] + a * x2[tid];\n",
    "}\n",
    "''', 'my_func')\n",
    "\n",
    "x1 = cp.arange(25, dtype=cp.complex64).reshape(5,5)\n",
    "x2 = 1j*cp.arange(25, dtype=cp.complex64).reshape(5,5)\n",
    "y = cp.zeros((5,5), dtype=cp.complex64)\n",
    "\n",
    "complex_kernel((1,),(25,),(x1, x2,y,cp.float32(2.0)))\n",
    "print(y)\n",
    "\n",
    "#output:\n",
    "#[[ 0. +0.j  1. +2.j  2. +4.j  3. +6.j  4. +8.j]\n",
    "#[ 5.+10.j  6.+12.j  7.+14.j  8.+16.j  9.+18.j]\n",
    "#[10.+20.j 11.+22.j 12.+24.j 13.+26.j 14.+28.j]\n",
    "#[15.+30.j 16.+32.j 17.+34.j 18.+36.j 19.+38.j]\n",
    "#[20.+40.j 21.+42.j 22.+44.j 23.+46.j 24.+48.j]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This also produced the same output:\n",
    "complex_kernel((5,), (5,), (x1, x2, y, cp.float32(2.0)))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Kernel Attributes ##################\n",
    "print(\"max_dynamic_shared_size_bytes: \", complex_kernel.max_dynamic_shared_size_bytes )\n",
    "\n",
    "print(\"max_threads_per_block: \", complex_kernel.max_threads_per_block )\n",
    "\n",
    "print(\"attributes: \",complex_kernel.attributes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Modules \n",
    "\n",
    "- The **RawModule** class is used to defining a large raw CUDA C source or loading an existing CUDA binary.\n",
    "- It is initialized by a CUDA C source code having a several kernels (functions) such that needed kernels are retrieved by calling the **get_function()** method.\n",
    "\n",
    "```python\n",
    "import cupy as cp\n",
    "loaded_from_source = r'''\n",
    "extern \"C\" {\n",
    "__global__ void test_sum(const float* A, const float* B, float* C, int N)\n",
    " { \n",
    "    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(tid < N)\n",
    "    {\n",
    "      C[tid] = A[tid] + B[tid]; \n",
    "    }\n",
    " }\n",
    " __global__ void test_multiply(const float* A, const float* B, float* C, int N )\n",
    " {\n",
    "    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(tid < N)\n",
    "    {\n",
    "        C[tid] = A[tid] * B[tid];\n",
    "    }\n",
    " }\n",
    "}'''\n",
    "```\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "load_raw_module = r'''\n",
    "extern \"C\" {\n",
    "__global__ void sum_ker(const float* a, const float* b, float* c)\n",
    " { \n",
    "    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    c[tid] = a[tid] + b[tid]; \n",
    "    \n",
    " }\n",
    " __global__ void multiply_ker(const float* a, const float* b, float* c )\n",
    " {\n",
    "    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    c[tid] = a[tid] * b[tid];\n",
    " }\n",
    "}'''\n",
    "\n",
    "module = cp.RawModule(code = load_raw_module)\n",
    "\n",
    "ker_sum = module.get_function('sum_ker')\n",
    "ker_times = module.get_function('multiply_ker')\n",
    "\n",
    "a = cp.arange(25, dtype=cp.float32).reshape(5,5)\n",
    "b = cp.ones((5,5), dtype=cp.float32)\n",
    "c = cp.zeros((5,5), dtype=cp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the above cell before runing this cell\n",
    "ker_sum((1,),(25,), (a,b,c))\n",
    "print(c)\n",
    "\n",
    "#output:\n",
    "#[[ 1.  2.  3.  4.  5.]\n",
    "#[ 6.  7.  8.  9. 10.]\n",
    "#[11. 12. 13. 14. 15.]\n",
    "#[16. 17. 18. 19. 20.]\n",
    "#[21. 22. 23. 24. 25.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ker_times((5,),(5,),(a,b,c))\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Fusion\n",
    "\n",
    "- Kernel fusion is a decorator that fuses functions. It can be used to define an elementwise or reduction kernels easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "@cp.fuse(kernel_name='dnnlayerNode')\n",
    "def dnnlayerNode(x, w, bias):\n",
    "    return  (x * w) + bias\n",
    "    \n",
    "x = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "w = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "bias = cp.array([-0.5], dtype=cp.float32)\n",
    "         \n",
    "z = dnnlayerNode(x,w,bias)\n",
    "print(z)\n",
    "\n",
    "#output:\n",
    "#[[-0.5  0.5  3.5]\n",
    "#[ 8.5 15.5 24.5]\n",
    "#[35.5 48.5 63.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "<img src=\"../images/cupy_summary.png\" width=\"80%\" height=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Task\n",
    "\n",
    "In this section, you are expected to click on the **Serial code Lab Assignment** link and proceed to Lab 2. In this lab you will find three python serial code functions. You are required to revise the **pair_gpu** function and make it run on the GPU, and likewise do a few modifications on the **main** function.\n",
    "\n",
    "## <div style=\"text-align:center; color:#FF0000; border:3px solid red;height:80px;\"> <b><br/> [Serial Code Lab Assignment](serial_RDF.ipynb) </b> </div>\n",
    "\n",
    " \n",
    "---\n",
    "\n",
    "\n",
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, it is recommend you go to your browsers File menu (not the Jupyter notebook file menu) and save the complete web page.  This will ensure the images are copied down as well. You can also execute the following cell block to create a zip-file of the files you've been working on, and download it with the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "rm -f nways_files.zip\n",
    "zip -r nways_files.zip *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**After** executing the above zip command, you should be able to download the zip file [here](../nways_files.zip).\n",
    "\n",
    "**IMPORTANT**: Please click on **HOME** to go back to the main notebook for *N ways of GPU programming for MD* code.\n",
    "\n",
    "---\n",
    "\n",
    "# <p style=\"text-align:center;border:3px; border-style:solid; border-color:#FF0000  ; padding: 1em\"> <a href=../../../nways_MD_start_python.ipynb>HOME</a></p>\n",
    "\n",
    "---\n",
    "\n",
    "# Links and Resources\n",
    "\n",
    "[NVIDIA Nsight System](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "[CUDA Toolkit Download](https://developer.nvidia.com/cuda-downloads)\n",
    "\n",
    "**NOTE**: To be able to see the Nsight System profiler output, please download Nsight System latest version from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "Don't forget to check out additional [OpenACC Resources](https://www.openacc.org/resources) and join our [OpenACC Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "##  References\n",
    "- https://docs.cupy.dev/en/stable/\n",
    "- https://cupy.dev/\n",
    "- CuPy Documentation Release 8.5.0, Preferred Networks, inc. and Preferred Infrastructure inc., Feb 26, 2021.\n",
    "- Bhaumik Vaidya, Hands-On GPU-Accelerated Computer Vision with OpenCV and CUDA, Packt Publishing, 2018.\n",
    "- Crissman Loomis and Emilio Castillo, CuPy Overview: NumPy Syntax Computation with Advanced CUDA Features, GTC Digital March, March 2020.\n",
    "- https://www.gpuhackathons.org/technical-resources\n",
    "- https://rapids.ai/start.html\n",
    "\n",
    "--- \n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by NVIDIA Corporation under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
